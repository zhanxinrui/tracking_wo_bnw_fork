{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DfPPQ6ztJhv4"
   },
   "source": [
    "# Faster R-CNN with FPN training on MOT17\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "yxXBbTJYqi5t",
    "outputId": "b1d4cc04-4746-4318-e70f-da03f2189abc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-WXLwePV5ieP"
   },
   "source": [
    "\n",
    "## Download torchvision and coco\n",
    "\n",
    "First, we need to install `pycocotools`. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union.\n",
    "\n",
    "\n",
    "In `references/detection/,` we have a number of helper functions to simplify training and evaluating detection models.\n",
    "Here, we will use `references/detection/engine.py`, `references/detection/utils.py` and `references/detection/transforms.py`.\n",
    "\n",
    "Let's copy those files (and their dependencies) in here so that they are available in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DBIoe_tHTQgV",
    "outputId": "38e69779-ad0d-491d-b714-ce0fedcc5a15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%shell` not found.\n"
     ]
    }
   ],
   "source": [
    "%%shell\n",
    "\n",
    "# Install pycocotools\n",
    "git clone https://github.com/cocodataset/cocoapi.git\n",
    "cd cocoapi/PythonAPI\n",
    "python setup.py build_ext install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "id": "UYDb7PBw55b-",
    "outputId": "6ef6f493-5414-4a5b-8d49-53cc2f1e2592"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'vision'...\n",
      "remote: Enumerating objects: 6171, done.\u001b[K\n",
      "Receiving objects:   0% (1/6171)   \r",
      "Receiving objects:   1% (62/6171)   \r",
      "Receiving objects:   2% (124/6171)   \r",
      "Receiving objects:   3% (186/6171)   \r",
      "Receiving objects:   4% (247/6171)   \r",
      "Receiving objects:   5% (309/6171)   \r",
      "Receiving objects:   6% (371/6171)   \r",
      "Receiving objects:   7% (432/6171)   \r",
      "Receiving objects:   8% (494/6171)   \r",
      "Receiving objects:   9% (556/6171)   \r",
      "Receiving objects:  10% (618/6171)   \r",
      "Receiving objects:  11% (679/6171)   \r",
      "Receiving objects:  12% (741/6171)   \r",
      "Receiving objects:  13% (803/6171)   \r",
      "Receiving objects:  14% (864/6171)   \r",
      "Receiving objects:  15% (926/6171)   \r",
      "Receiving objects:  16% (988/6171)   \r",
      "Receiving objects:  17% (1050/6171)   \r",
      "Receiving objects:  18% (1111/6171)   \r",
      "Receiving objects:  19% (1173/6171)   \r",
      "Receiving objects:  20% (1235/6171)   \r",
      "Receiving objects:  21% (1296/6171)   \r",
      "Receiving objects:  22% (1358/6171)   \r",
      "Receiving objects:  23% (1420/6171)   \r",
      "Receiving objects:  24% (1482/6171)   \r",
      "Receiving objects:  25% (1543/6171)   \r",
      "Receiving objects:  26% (1605/6171)   \r",
      "Receiving objects:  27% (1667/6171)   \r",
      "Receiving objects:  28% (1728/6171)   \r",
      "Receiving objects:  29% (1790/6171)   \r",
      "Receiving objects:  30% (1852/6171)   \r",
      "Receiving objects:  31% (1914/6171)   \r",
      "Receiving objects:  32% (1975/6171)   \r",
      "Receiving objects:  33% (2037/6171)   \r",
      "Receiving objects:  34% (2099/6171)   \r",
      "Receiving objects:  35% (2160/6171)   \r",
      "Receiving objects:  36% (2222/6171)   \r",
      "Receiving objects:  37% (2284/6171)   \r",
      "Receiving objects:  38% (2345/6171)   \r",
      "Receiving objects:  39% (2407/6171)   \r",
      "Receiving objects:  40% (2469/6171)   \r",
      "Receiving objects:  41% (2531/6171)   \r",
      "Receiving objects:  42% (2592/6171)   \r",
      "Receiving objects:  43% (2654/6171)   \r",
      "Receiving objects:  44% (2716/6171)   \r",
      "Receiving objects:  45% (2777/6171)   \r",
      "Receiving objects:  46% (2839/6171)   \r",
      "Receiving objects:  47% (2901/6171)   \r",
      "Receiving objects:  48% (2963/6171)   \r",
      "Receiving objects:  49% (3024/6171)   \r",
      "Receiving objects:  50% (3086/6171)   \r",
      "Receiving objects:  51% (3148/6171)   \r",
      "Receiving objects:  52% (3209/6171)   \r",
      "Receiving objects:  53% (3271/6171)   \r",
      "Receiving objects:  54% (3333/6171)   \r",
      "Receiving objects:  55% (3395/6171)   \r",
      "Receiving objects:  56% (3456/6171)   \r",
      "Receiving objects:  57% (3518/6171)   \r",
      "Receiving objects:  58% (3580/6171)   \r",
      "Receiving objects:  59% (3641/6171)   \r",
      "Receiving objects:  60% (3703/6171)   \r",
      "Receiving objects:  61% (3765/6171)   \r",
      "Receiving objects:  62% (3827/6171)   \r",
      "Receiving objects:  63% (3888/6171)   \r",
      "Receiving objects:  64% (3950/6171)   \r",
      "Receiving objects:  65% (4012/6171)   \r",
      "Receiving objects:  66% (4073/6171)   \r",
      "Receiving objects:  67% (4135/6171)   \r",
      "Receiving objects:  68% (4197/6171)   \r",
      "Receiving objects:  69% (4258/6171)   \r",
      "Receiving objects:  70% (4320/6171)   \r",
      "Receiving objects:  71% (4382/6171)   \r",
      "Receiving objects:  72% (4444/6171)   \r",
      "Receiving objects:  73% (4505/6171)   \r",
      "Receiving objects:  74% (4567/6171)   \r",
      "Receiving objects:  75% (4629/6171)   \r",
      "Receiving objects:  76% (4690/6171)   \r",
      "Receiving objects:  77% (4752/6171)   \r",
      "Receiving objects:  78% (4814/6171)   \r",
      "Receiving objects:  79% (4876/6171)   \r",
      "Receiving objects:  80% (4937/6171)   \r",
      "Receiving objects:  81% (4999/6171)   \r",
      "Receiving objects:  82% (5061/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects:  83% (5122/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects:  84% (5184/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects:  85% (5246/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects:  86% (5308/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects:  87% (5369/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects:  88% (5431/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects:  89% (5493/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects:  90% (5554/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects:  91% (5616/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "remote: Total 6171 (delta 0), reused 0 (delta 0), pack-reused 6171\n",
      "Receiving objects:  92% (5678/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects:  93% (5740/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects:  94% (5801/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects:  95% (5863/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects:  96% (5925/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects:  97% (5986/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects:  98% (6048/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects:  99% (6110/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects: 100% (6171/6171), 9.41 MiB | 18.81 MiB/s   \r",
      "Receiving objects: 100% (6171/6171), 9.60 MiB | 18.98 MiB/s, done.\n",
      "Resolving deltas:   0% (0/4122)   \r",
      "Resolving deltas:   1% (42/4122)   \r",
      "Resolving deltas:   2% (94/4122)   \r",
      "Resolving deltas:   3% (126/4122)   \r",
      "Resolving deltas:   4% (173/4122)   \r",
      "Resolving deltas:   5% (207/4122)   \r",
      "Resolving deltas:   6% (250/4122)   \r",
      "Resolving deltas:   7% (300/4122)   \r",
      "Resolving deltas:   8% (331/4122)   \r",
      "Resolving deltas:   9% (394/4122)   \r",
      "Resolving deltas:  11% (461/4122)   \r",
      "Resolving deltas:  12% (496/4122)   \r",
      "Resolving deltas:  13% (544/4122)   \r",
      "Resolving deltas:  14% (583/4122)   \r",
      "Resolving deltas:  15% (620/4122)   \r",
      "Resolving deltas:  16% (660/4122)   \r",
      "Resolving deltas:  17% (729/4122)   \r",
      "Resolving deltas:  18% (746/4122)   \r",
      "Resolving deltas:  19% (790/4122)   \r",
      "Resolving deltas:  20% (825/4122)   \r",
      "Resolving deltas:  21% (868/4122)   \r",
      "Resolving deltas:  22% (912/4122)   \r",
      "Resolving deltas:  23% (951/4122)   \r",
      "Resolving deltas:  24% (1010/4122)   \r",
      "Resolving deltas:  25% (1037/4122)   \r",
      "Resolving deltas:  26% (1073/4122)   \r",
      "Resolving deltas:  27% (1129/4122)   \r",
      "Resolving deltas:  28% (1159/4122)   \r",
      "Resolving deltas:  29% (1207/4122)   \r",
      "Resolving deltas:  31% (1286/4122)   \r",
      "Resolving deltas:  32% (1340/4122)   \r",
      "Resolving deltas:  33% (1372/4122)   \r",
      "Resolving deltas:  34% (1404/4122)   \r",
      "Resolving deltas:  35% (1447/4122)   \r",
      "Resolving deltas:  36% (1487/4122)   \r",
      "Resolving deltas:  37% (1526/4122)   \r",
      "Resolving deltas:  38% (1572/4122)   \r",
      "Resolving deltas:  39% (1625/4122)   \r",
      "Resolving deltas:  40% (1657/4122)   \r",
      "Resolving deltas:  43% (1811/4122)   \r",
      "Resolving deltas:  44% (1815/4122)   \r",
      "Resolving deltas:  45% (1893/4122)   \r",
      "Resolving deltas:  46% (1920/4122)   \r",
      "Resolving deltas:  47% (1946/4122)   \r",
      "Resolving deltas:  48% (1984/4122)   \r",
      "Resolving deltas:  49% (2020/4122)   \r",
      "Resolving deltas:  50% (2061/4122)   \r",
      "Resolving deltas:  51% (2103/4122)   \r",
      "Resolving deltas:  52% (2149/4122)   \r",
      "Resolving deltas:  53% (2193/4122)   \r",
      "Resolving deltas:  54% (2229/4122)   \r",
      "Resolving deltas:  55% (2286/4122)   \r",
      "Resolving deltas:  56% (2317/4122)   \r",
      "Resolving deltas:  57% (2363/4122)   \r",
      "Resolving deltas:  58% (2391/4122)   \r",
      "Resolving deltas:  59% (2432/4122)   \r",
      "Resolving deltas:  60% (2476/4122)   \r",
      "Resolving deltas:  61% (2535/4122)   \r",
      "Resolving deltas:  62% (2577/4122)   \r",
      "Resolving deltas:  63% (2608/4122)   \r",
      "Resolving deltas:  66% (2734/4122)   \r",
      "Resolving deltas:  67% (2766/4122)   \r",
      "Resolving deltas:  68% (2804/4122)   \r",
      "Resolving deltas:  69% (2845/4122)   \r",
      "Resolving deltas:  70% (2886/4122)   \r",
      "Resolving deltas:  71% (2950/4122)   \r",
      "Resolving deltas:  72% (2968/4122)   \r",
      "Resolving deltas:  73% (3020/4122)   \r",
      "Resolving deltas:  74% (3051/4122)   \r",
      "Resolving deltas:  75% (3092/4122)   \r",
      "Resolving deltas:  76% (3152/4122)   \r",
      "Resolving deltas:  77% (3175/4122)   \r",
      "Resolving deltas:  78% (3229/4122)   \r",
      "Resolving deltas:  79% (3277/4122)   \r",
      "Resolving deltas:  80% (3309/4122)   \r",
      "Resolving deltas:  81% (3344/4122)   \r",
      "Resolving deltas:  82% (3383/4122)   \r",
      "Resolving deltas:  83% (3423/4122)   \r",
      "Resolving deltas:  84% (3472/4122)   \r",
      "Resolving deltas:  85% (3504/4122)   \r",
      "Resolving deltas:  86% (3553/4122)   \r",
      "Resolving deltas:  87% (3589/4122)   \r",
      "Resolving deltas:  88% (3632/4122)   \r",
      "Resolving deltas:  89% (3670/4122)   \r",
      "Resolving deltas:  90% (3712/4122)   \r",
      "Resolving deltas:  91% (3764/4122)   \r",
      "Resolving deltas:  92% (3794/4122)   \r",
      "Resolving deltas:  93% (3835/4122)   \r",
      "Resolving deltas:  94% (3875/4122)   \r",
      "Resolving deltas:  95% (3951/4122)   \r",
      "Resolving deltas:  96% (3963/4122)   \r",
      "Resolving deltas:  97% (4000/4122)   \r",
      "Resolving deltas:  98% (4040/4122)   \r",
      "Resolving deltas:  99% (4084/4122)   \r",
      "Resolving deltas: 100% (4122/4122)   \r",
      "Resolving deltas: 100% (4122/4122), done.\n",
      "Note: checking out 'v0.3.0'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by performing another checkout.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -b with the checkout command again. Example:\n",
      "\n",
      "  git checkout -b <new-branch-name>\n",
      "\n",
      "HEAD is now at be37608 version check against PyTorch's CUDA version\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "\n",
    "# Download TorchVision repo to use some files from\n",
    "# references/detection\n",
    "git clone https://github.com/pytorch/vision.git\n",
    "cd vision\n",
    "git checkout v0.3.0\n",
    "\n",
    "cp references/detection/utils.py ../\n",
    "cp references/detection/transforms.py ../\n",
    "cp references/detection/coco_eval.py ../\n",
    "cp references/detection/engine.py ../\n",
    "cp references/detection/coco_utils.py ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bX0rqK-A3Nbl"
   },
   "source": [
    "### Download MOT 17 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_t4TBwhHTdkd"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "# download the MOT17 detection challenge\n",
    "wget https://motchallenge.net/data/MOT17Det.zip .\n",
    "# extract it in the current folder\n",
    "unzip MOT17Det.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Miwl9_ESZzZY"
   },
   "source": [
    "Let's have a look at the dataset and how it is layed down.\n",
    "\n",
    "The data is structured as follows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ct3TEoAwZphs"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "ls\n",
    "ls train\n",
    "ls test\n",
    "\n",
    "ls train/MOT17-02/\n",
    "ls train/MOT17-02/img1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdIDvMw_Z4lY"
   },
   "source": [
    "\n",
    "Here is one example of an image in the dataset, with its corresponding instance segmentation mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDjuVFgexFfh"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open('train/MOT17-02/img1/000001.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Sd4jlGp2eLm"
   },
   "source": [
    "## Defining the Dataset\n",
    "\n",
    "The [torchvision reference scripts for training object detection, instance segmentation and person keypoint detection](https://github.com/pytorch/vision/tree/v0.3.0/references/detection) allows for easily supporting adding new custom datasets.\n",
    "The dataset should inherit from the standard `torch.utils.data.Dataset` class, and implement `__len__` and `__getitem__`.\n",
    "\n",
    "The only specificity that we require is that the dataset `__getitem__` should return:\n",
    "\n",
    "* image: a PIL Image of size (H, W)\n",
    "* target: a dict containing the following fields\n",
    "    * `boxes` (`FloatTensor[N, 4]`): the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`\n",
    "    * `labels` (`Int64Tensor[N]`): the label for each bounding box\n",
    "    * `image_id` (`Int64Tensor[1]`): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
    "    * `area` (`Tensor[N]`): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
    "    * `iscrowd` (`UInt8Tensor[N]`): instances with `iscrowd=True` will be ignored during evaluation.\n",
    "    * (optionally) `masks` (`UInt8Tensor[N, H, W]`): The segmentation masks for each one of the objects\n",
    "    * (optionally) `keypoints` (`FloatTensor[N, K, 3]`): For each one of the `N` objects, it contains the `K` keypoints in `[x, y, visibility]` format, defining the object. `visibility=0` means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt `references/detection/transforms.py` for your new keypoint representation\n",
    "\n",
    "If your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools.\n",
    "\n",
    "Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a `get_height_and_width` method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via `__getitem__` , which loads the image in memory and is slower than if a custom method is provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C9Ee5NV54Dmj"
   },
   "source": [
    "So each image has a corresponding segmentation mask, where each color correspond to a different instance. Let's write a `torch.utils.data.Dataset` class for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3R9PRiGaasyM"
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import csv\n",
    "import os\n",
    "import os.path as osp\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "\n",
    "\n",
    "class MOT17ObjDetect(torch.utils.data.Dataset):\n",
    "    \"\"\" Data class for the Multiple Object Tracking Dataset\n",
    "      In order to be compatiable with torchvision\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, transforms=None, vis_threshold=0.25):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self._vis_threshold = vis_threshold\n",
    "        self._classes = ('background', 'pedestrian')#only two classes\n",
    "        self._img_paths = []\n",
    "\n",
    "        for f in os.listdir(root):\n",
    "            path = os.path.join(root, f)\n",
    "            config_file = os.path.join(path, 'seqinfo.ini')\n",
    "\n",
    "            assert os.path.exists(config_file), \\\n",
    "                'Path does not exist: {}'.format(config_file)\n",
    "\n",
    "            config = configparser.ConfigParser()\n",
    "            config.read(config_file)\n",
    "            seq_len = int(config['Sequence']['seqLength'])\n",
    "            im_width = int(config['Sequence']['imWidth'])\n",
    "            im_height = int(config['Sequence']['imHeight'])\n",
    "            im_ext = config['Sequence']['imExt']\n",
    "            im_dir = config['Sequence']['imDir']\n",
    "\n",
    "            _imDir = os.path.join(path, im_dir)\n",
    "\n",
    "            for i in range(1, seq_len + 1):\n",
    "                img_path = os.path.join(_imDir, f\"{i:06d}{im_ext}\")\n",
    "                assert os.path.exists(img_path), \\\n",
    "                    'Path does not exist: {img_path}'\n",
    "                # self._img_paths.append((img_path, im_width, im_height))\n",
    "                self._img_paths.append(img_path)\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return len(self._classes)\n",
    "\n",
    "    def _get_annotation(self, idx):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        if 'test' in self.root:\n",
    "            \n",
    "            num_objs = 0\n",
    "            boxes = torch.zeros((num_objs, 4), dtype=torch.float32)\n",
    "\n",
    "            return {'boxes': boxes,\n",
    "                'labels': torch.ones((num_objs,), dtype=torch.int64),\n",
    "                'image_id': torch.tensor([idx]),\n",
    "                'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "                'iscrowd': torch.zeros((num_objs,), dtype=torch.int64),\n",
    "                'visibilities': torch.zeros((num_objs), dtype=torch.float32)}\n",
    "                \n",
    "        img_path = self._img_paths[idx]\n",
    "        file_index = int(os.path.basename(img_path).split('.')[0])\n",
    "\n",
    "        gt_file = os.path.join(os.path.dirname(\n",
    "            os.path.dirname(img_path)), 'gt', 'gt.txt')\n",
    "\n",
    "        assert os.path.exists(gt_file), \\\n",
    "            'GT file does not exist: {}'.format(gt_file)\n",
    "\n",
    "        bounding_boxes = []\n",
    "\n",
    "        with open(gt_file, \"r\") as inf:\n",
    "            reader = csv.reader(inf, delimiter=',')\n",
    "            for row in reader:\n",
    "                visibility = float(row[8])\n",
    "                if int(row[0]) == file_index and int(row[6]) == 1 and int(row[7]) == 1 and visibility >= self._vis_threshold:\n",
    "                    bb = {}\n",
    "                    bb['bb_left'] = int(row[2])\n",
    "                    bb['bb_top'] = int(row[3])\n",
    "                    bb['bb_width'] = int(row[4])\n",
    "                    bb['bb_height'] = int(row[5])\n",
    "                    bb['visibility'] = float(row[8])\n",
    "\n",
    "                    bounding_boxes.append(bb)\n",
    "\n",
    "        num_objs = len(bounding_boxes)\n",
    "\n",
    "        boxes = torch.zeros((num_objs, 4), dtype=torch.float32)\n",
    "        visibilities = torch.zeros((num_objs), dtype=torch.float32)\n",
    "        \n",
    "        for i, bb in enumerate(bounding_boxes):\n",
    "            # Make pixel indexes 0-based, should already be 0-based (or not)\n",
    "            x1 = bb['bb_left'] - 1\n",
    "            y1 = bb['bb_top'] - 1\n",
    "            # This -1 accounts for the width (width of 1 x1=x2)\n",
    "            x2 = x1 + bb['bb_width'] - 1\n",
    "            y2 = y1 + bb['bb_height'] - 1\n",
    "\n",
    "            boxes[i, 0] = x1\n",
    "            boxes[i, 1] = y1\n",
    "            boxes[i, 2] = x2\n",
    "            boxes[i, 3] = y2\n",
    "            visibilities[i] = bb['visibility']\n",
    "            \n",
    "        return {'boxes': boxes,\n",
    "                'labels': torch.ones((num_objs,), dtype=torch.int64),\n",
    "                'image_id': torch.tensor([idx]),\n",
    "                'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "                'iscrowd': torch.zeros((num_objs,), dtype=torch.int64),\n",
    "                'visibilities': visibilities,}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = self._img_paths[idx]\n",
    "        # mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        target = self._get_annotation(idx)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._img_paths)\n",
    "    \n",
    "    def write_results_files(self, results, output_dir):\n",
    "        \"\"\"Write the detections in the format for MOT17Det sumbission\n",
    "\n",
    "        all_boxes[image] = N x 5 array of detections in (x1, y1, x2, y2, score)\n",
    "\n",
    "        Each file contains these lines:\n",
    "        <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <conf>, <x>, <y>, <z>\n",
    "\n",
    "        Files to sumbit:\n",
    "        ./MOT17-01.txt\n",
    "        ./MOT17-02.txt\n",
    "        ./MOT17-03.txt\n",
    "        ./MOT17-04.txt\n",
    "        ./MOT17-05.txt\n",
    "        ./MOT17-06.txt\n",
    "        ./MOT17-07.txt\n",
    "        ./MOT17-08.txt\n",
    "        ./MOT17-09.txt\n",
    "        ./MOT17-10.txt\n",
    "        ./MOT17-11.txt\n",
    "        ./MOT17-12.txt\n",
    "        ./MOT17-13.txt\n",
    "        ./MOT17-14.txt\n",
    "        \"\"\"\n",
    "\n",
    "        #format_str = \"{}, -1, {}, {}, {}, {}, {}, -1, -1, -1\"\n",
    "\n",
    "        files = {}\n",
    "        for image_id, res in results.items():\n",
    "            path = self._img_paths[image_id]\n",
    "            img1, name = osp.split(path)\n",
    "            # get image number out of name\n",
    "            frame = int(name.split('.')[0])\n",
    "            # smth like /train/MOT17-09-FRCNN or /train/MOT17-09\n",
    "            tmp = osp.dirname(img1)\n",
    "            # get the folder name of the sequence and split it\n",
    "            tmp = osp.basename(tmp).split('-')\n",
    "            # Now get the output name of the file\n",
    "            out = tmp[0]+'-'+tmp[1]+'.txt'\n",
    "            outfile = osp.join(output_dir, out)\n",
    "\n",
    "            # check if out in keys and create empty list if not\n",
    "            if outfile not in files.keys():\n",
    "                files[outfile] = []\n",
    "\n",
    "            for box, score in zip(res['boxes'], res['scores']):\n",
    "                x1 = box[0].item()\n",
    "                y1 = box[1].item()\n",
    "                x2 = box[2].item()\n",
    "                y2 = box[3].item()\n",
    "                files[outfile].append(\n",
    "                    [frame, -1, x1, y1, x2 - x1, y2 - y1, score.item(), -1, -1, -1])\n",
    "\n",
    "        for k, v in files.items():\n",
    "            with open(k, \"w\") as of:\n",
    "                writer = csv.writer(of, delimiter=',')\n",
    "                for d in v:\n",
    "                    writer.writerow(d)\n",
    "\n",
    "    def print_eval(self, results, ovthresh=0.5):\n",
    "        \"\"\"Evaluates the detections (not official!!)\n",
    "\n",
    "        all_boxes[cls][image] = N x 5 array of detections in (x1, y1, x2, y2, score)\n",
    "        \"\"\"\n",
    "\n",
    "        if 'test' in self.root:\n",
    "            print('No GT data available for evaluation.')\n",
    "            return\n",
    "            \n",
    "        # Lists for tp and fp in the format tp[cls][image]\n",
    "        tp = [[] for _ in range(len(self._img_paths))]\n",
    "        fp = [[] for _ in range(len(self._img_paths))]\n",
    "\n",
    "        npos = 0\n",
    "        gt = []\n",
    "        gt_found = []\n",
    "\n",
    "        for idx in range(len(self._img_paths)):\n",
    "            annotation = self._get_annotation(idx)\n",
    "            bbox = annotation['boxes'][annotation['visibilities'].gt(self._vis_threshold)]\n",
    "            found = np.zeros(bbox.shape[0])\n",
    "            gt.append(bbox.cpu().numpy())\n",
    "            gt_found.append(found)\n",
    "\n",
    "            npos += found.shape[0]\n",
    "\n",
    "        # Loop through all images\n",
    "        # for res in results:\n",
    "        for im_index, (im_gt, found) in enumerate(zip(gt, gt_found)):\n",
    "            # Loop through dets an mark TPs and FPs\n",
    "            \n",
    "            # im_index = res['image_id'].item()\n",
    "            # im_det = results['boxes']\n",
    "            # annotation = self._get_annotation(im_index)\n",
    "            # im_gt = annotation['boxes'][annotation['visibilities'].gt(0.5)].cpu().numpy()\n",
    "            # found = np.zeros(im_gt.shape[0])\n",
    "            \n",
    "            im_det = results[im_index]['boxes'].cpu().numpy()\n",
    "\n",
    "            im_tp = np.zeros(len(im_det))\n",
    "            im_fp = np.zeros(len(im_det))\n",
    "            for i, d in enumerate(im_det):\n",
    "                ovmax = -np.inf\n",
    "\n",
    "                if im_gt.size > 0:\n",
    "                    # compute overlaps\n",
    "                    # intersection\n",
    "                    ixmin = np.maximum(im_gt[:, 0], d[0])\n",
    "                    iymin = np.maximum(im_gt[:, 1], d[1])\n",
    "                    ixmax = np.minimum(im_gt[:, 2], d[2])\n",
    "                    iymax = np.minimum(im_gt[:, 3], d[3])\n",
    "                    iw = np.maximum(ixmax - ixmin + 1., 0.)\n",
    "                    ih = np.maximum(iymax - iymin + 1., 0.)\n",
    "                    inters = iw * ih\n",
    "                    \n",
    "                    # union\n",
    "                    uni = ((d[2] - d[0] + 1.) * (d[3] - d[1] + 1.) +\n",
    "                            (im_gt[:, 2] - im_gt[:, 0] + 1.) *\n",
    "                            (im_gt[:, 3] - im_gt[:, 1] + 1.) - inters)\n",
    "\n",
    "                    overlaps = inters / uni\n",
    "                    ovmax = np.max(overlaps)\n",
    "                    jmax = np.argmax(overlaps)\n",
    "\n",
    "                if ovmax > ovthresh:\n",
    "                    if found[jmax] == 0:\n",
    "                        im_tp[i] = 1.\n",
    "                        found[jmax] = 1.\n",
    "                    else:\n",
    "                        im_fp[i] = 1.\n",
    "                else:\n",
    "                    im_fp[i] = 1.\n",
    "\n",
    "            tp[im_index] = im_tp\n",
    "            fp[im_index] = im_fp\n",
    "\n",
    "        # Flatten out tp and fp into a numpy array\n",
    "        i = 0\n",
    "        for im in tp:\n",
    "            if type(im) != type([]):\n",
    "                i += im.shape[0]\n",
    "\n",
    "        tp_flat = np.zeros(i)\n",
    "        fp_flat = np.zeros(i)\n",
    "\n",
    "        i = 0\n",
    "        for tp_im, fp_im in zip(tp, fp):\n",
    "            if type(tp_im) != type([]):\n",
    "                s = tp_im.shape[0]\n",
    "                tp_flat[i:s+i] = tp_im\n",
    "                fp_flat[i:s+i] = fp_im\n",
    "                i += s\n",
    "\n",
    "        tp = np.cumsum(tp_flat)\n",
    "        fp = np.cumsum(fp_flat)\n",
    "        rec = tp / float(npos)\n",
    "        # avoid divide by zero in case the first detection matches a difficult\n",
    "        # ground truth (probably not needed in my code but doesn't harm if left)\n",
    "        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "        tmp = np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "\n",
    "        # correct AP calculation\n",
    "        # first append sentinel values at the end\n",
    "        mrec = np.concatenate(([0.], rec, [1.]))\n",
    "        mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "        # compute the precision envelope\n",
    "        for i in range(mpre.size - 1, 0, -1):\n",
    "            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "        # to calculate area under PR curve, look for points\n",
    "        # where X axis (recall) changes value\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "        # and sum (\\Delta recall) * prec\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "\n",
    "        tp, fp, prec, rec, ap = np.max(tp), np.max(fp), prec[-1], np.max(rec), ap\n",
    "        \n",
    "        print(f\"AP: {ap} Prec: {prec} Rec: {rec} TP: {tp} FP: {fp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZNTZGnUitoE"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import transforms as T\n",
    "\n",
    "dataset = MOT17ObjDetect('train')\n",
    "img, target = dataset[0]\n",
    "\n",
    "def plot(img, boxes):\n",
    "  fig, ax = plt.subplots(1, dpi=96)\n",
    "\n",
    "  img = img.mul(255).permute(1, 2, 0).byte().numpy()\n",
    "  width, height, _ = img.shape\n",
    "    \n",
    "  ax.imshow(img, cmap='gray')\n",
    "  fig.set_size_inches(width / 80, height / 80)\n",
    "\n",
    "  for box in boxes:\n",
    "      rect = plt.Rectangle(\n",
    "        (box[0], box[1]),\n",
    "        box[2] - box[0],\n",
    "        box[3] - box[1],\n",
    "        fill=False,\n",
    "        linewidth=1.0)\n",
    "      ax.add_patch(rect)\n",
    "\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "img, target = T.ToTensor()(img, target)\n",
    "plot(img, target['boxes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J6f3ZOTJ4Km9"
   },
   "source": [
    "That's all for the dataset. Let's see how the outputs are structured for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YjNHjVMOyYlH",
    "outputId": "3cb8d422-4673-4703-e6d0-89797ca19088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "# detection使用rpn\n",
    "def get_detection_model(num_classes):\n",
    "    #     load an instance segmentation model pre-trained on COCO\n",
    "    # model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    # get the number of input features for the classifier\n",
    "    print('model.roi_heads.box_predictor:',model.roi_heads.box_predictor)#\n",
    "    '''\n",
    "    output:\n",
    "    model.roi_heads.box_predictor: FastRCNNPredictor(\n",
    "  (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
    "  (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
    ")\n",
    "    '''\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features#1024\n",
    "    # print(in_features.size)\n",
    "    # replace the pre-trained head with a new one\n",
    "    # model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.roi_heads.box_predictor = MaskRCNNPredictor(in_features, 512,num_classes)\n",
    "    model.roi_heads.nms_thresh = 0.3\n",
    "\n",
    "    return model\n",
    "print('passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YFJGJxk6XEs"
   },
   "source": [
    "DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l79ivkwKy357"
   },
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5dGaIezze3y"
   },
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = MOT17ObjDetect('train', get_transform(train=True))\n",
    "dataset_no_random = MOT17ObjDetect('train', get_transform(train=False))\n",
    "dataset_test = MOT17ObjDetect('test', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "data_loader_no_random = torch.utils.data.DataLoader(\n",
    "    dataset_no_random, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5yvZUprj4ZN"
   },
   "source": [
    "INIT MODEL AND OPTIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "id": "zoenkCj18C4h",
    "outputId": "b382c072-fa5c-4bee-c3f4-a825f5e21dcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.num_classes: 2\n",
      "model.roi_heads.box_predictor: FastRCNNPredictor(\n",
      "  (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
      "  (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-db9e07fc8244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset.num_classes:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_detection_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;31m#frcnn是1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"in_features:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 585\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MaskRCNNPredictor' object has no attribute 'cls_score'"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# get the model using our helper function\n",
    "print('dataset.num_classes:',dataset.num_classes)#2\n",
    "model = get_detection_model(dataset.num_classes)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features#frcnn是1024\n",
    "print(\"in_features:\",in_features)\n",
    "print('model',model)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "#model_state_dict = torch.load(f\"gdrive/My Drive/Colab Notebooks/faster_rcnn_fpn_training_mot_17/resnet50/model_epoch_27.model\")\n",
    "#model.load_state_dict(model_state_dict)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.00001,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=10,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAd56lt4kDxc"
   },
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "18MtwEFcxLxD"
   },
   "outputs": [],
   "source": [
    "def evaluate_and_write_result_files(model, data_loader):\n",
    "  model.eval()\n",
    "  results = {}\n",
    "  for imgs, targets in data_loader:\n",
    "    imgs = [img.to(device) for img in imgs]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(imgs)\n",
    "                                                                                                  \n",
    "    for pred, target in zip(preds, targets):\n",
    "        results[target['image_id'].item()] = {'boxes': pred['boxes'].cpu(),\n",
    "                                              'scores': pred['scores'].cpu()}\n",
    "\n",
    "  data_loader.dataset.print_eval(results)\n",
    "  data_loader.dataset.write_results_files(results, 'gdrive/My Drive/Colab Notebooks/faster_rcnn_fpn_training_mot_17/resnet50/')\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "evaluate_and_write_result_files(model, data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "at-h4OWK0aoc"
   },
   "outputs": [],
   "source": [
    "num_epochs = 27\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=200)\n",
    "    # update the learning rate\n",
    "    # lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    if epoch % 3 == 0:\n",
    "      evaluate_and_write_result_files(model, data_loader_no_random)\n",
    "      torch.save(model.state_dict(), f\"gdrive/My Drive/Colab Notebooks/faster_rcnn_fpn_training_mot_17/model_epoch_{epoch}.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6mYGFLxkO8F"
   },
   "source": [
    "QUALITATIVE TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHwIdxH76uPj"
   },
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "dataset = MOT17('train', get_transform(train=False))\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "for imgs, target in data_loader:\n",
    "    print(dataset._img_paths[0])\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model([imgs[0].to(device)])[0]\n",
    "    \n",
    "    plot(imgs[0], prediction['boxes'])\n",
    "    plot(imgs[0], target[0]['boxes'])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NaId3WIn2oKf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhoAZVhOeTIR"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "watch -n 13 -d nvidia-smi\n",
    "clear"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "““mask_rcnn_fpn_training_mot_17”",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:tracking_wo_bnw] *",
   "language": "python",
   "name": "conda-env-tracking_wo_bnw-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
